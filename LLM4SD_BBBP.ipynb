{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9b8df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76169ab1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15689642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-setup when running on Google Colab\n",
    "import os\n",
    "if 'google.colab' in str(get_ipython()) and not os.path.exists('/content/LLM4SD'):\n",
    "    !git clone -q https://github.com/mamintoosi-papers-codes/LLM4SD.git /content/LLM4SD\n",
    "    # !pip --quiet install -r /content/LLM4SD/requirements.txt\n",
    "    %cd LLM4SD\n",
    "    !curl -fsSL https://ollama.com/install.sh | sh  # نصب Ollama\n",
    "    !ollama pull falcon:7b  # دانلود مدل\n",
    "    !ollama run falcon:7b \"چرا آسمان آبی است؟\"  # اجرای مدل"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d69f11d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Step 1: Generating Prompt files...\n"
     ]
    }
   ],
   "source": [
    "# **Step 1:** Generate prior knowledge and data knowledge prompt files\n",
    "print(\"Processing Step 1: Generating Prompt files...\")\n",
    "%run create_prompt.py --task synthesize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1d404d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run create_prompt.py --task inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab74f455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Step 2: LLM for Scientific Synthesis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\m.amintoosi\\.conda\\envs\\pth-gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\m.amintoosi\\.conda\\envs\\pth-gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "\n",
      "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5945df05be4cdb81b2cec69960483c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\m.amintoosi\\.conda\\envs\\pth-gpu\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting bbbp_small dataset prior knowledge prompt ....\n",
      "Assumed you are an experienced chemist. Please come up with 20 rules that you think are very important to predict if a molecule is blood brain barrier permeable (BBBP). Each rule is either about the structure or property of molecules. Each rule starts with 'Calculate....' and don't explain, be short and within 5 words.\n",
      "\n",
      "Assumed you are an experienced chemist. Please come up with 20 rules that you think are very important to predict if a molecule is blood brain barrier permeable (BBBP). Each rule is either about the structure or property of molecules. Each rule starts with 'Calculate....' and don't explain, be short and within 5 words.\n",
      "1. Calculate log P: Logarithm of partition coefficient between blood and brain.\n",
      "2. Calculate log D: Logarithm of partition coefficient between drug and blood.\n",
      "3. Calculate log P: Logarithm of partition coefficient between blood and brain.\n",
      "4. Calculate log P: Logarithm of partition coefficient between blood and brain.\n",
      "5. Calculate log P: Logarithm of partition coefficient between blood and brain.\n",
      "6. Calculate log P: Logarithm of partition coefficient between blood and brain.\n",
      "7. Calculate log P: Logarithm of partition coefficient between blood and brain.\n",
      "8. Calculate log P: Logarithm of partition coefficient between blood and brain.\n",
      "9. Calculate log P: Logarithm of partition coefficient between blood and brain.\n",
      "10. Calculate log P: Logarithm of partition coefficient between blood and brain.\n",
      "11. Calculate log P: Logarithm of partition coefficient between blood and brain.\n",
      "12. Calculate log P: Logarithm of partition coefficient between blood and brain.\n",
      "13. Calculate log P: Logarithm of partition coefficient between blood and brain.\n",
      "14. Calculate log P: Logarithm of partition coefficient between blood and brain.\n",
      "15. Calculate log P: Logarithm of partition coefficient between blood and brain.\n",
      "16. Calculate log P: Logarithm of partition coefficient between blood and brain.\n",
      "17. Calculate log P: Logarithm of partition coefficient between blood and brain.\n",
      "18. Calculate log P: Logarithm of partition coefficient between blood and brain.\n",
      "19. Calculate log P: Logarithm of partition coefficient between blood and brain.\n",
      "20. Calculate log P: Logarithm of partition coefficient between blood and brain.\n",
      "Synthesize/Time elapsed: 104.15793132781982 seconds\n"
     ]
    }
   ],
   "source": [
    "# **Step 2:** Knowledge synthesis for BBBP dataset\n",
    "print(\"Processing Step 2: LLM for Scientific Synthesis\")\n",
    "%run synthesize.py --dataset bbbp --subtask \"\" --model falcon-7b --output_folder \"synthesize_model_response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fc6446b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Step 2: LLM for Scientific Synthesis\n",
      "Extracting bbbp_small dataset prior knowledge prompt ....\n",
      "Querying Ollama Model: falcon-7b\n",
      "1. Calculate log P: Logarithm of partitioning coefficient.\n",
      "2. Estimate BB: The expected BB value of the molecule based on its structure.\n",
      "3. Calculate lipophilic %: The proportion of polar and non-polar atoms in the molecule.\n",
      "4. Estimate pol: Calculate the polar index of the molecule.\n",
      "5. Calculate size: Estimate the size of the molecule.\n",
      "6. Calculate volume: Estimate the volume of the molecule.\n",
      "7. Estimate pol: Calculate the polar index of the molecule.\n",
      "8. Estimate BB: Estimate the BB value of the molecule based on its structure.\n",
      "9. Estimate lipo: Estimate the lipophilic content of the molecule.\n",
      "10. Evaluate NPS (nonpolar): Estimate the nonpolar nature of the molecule.\n",
      "11. Estimate BB: Estimate the BB value of the molecule based on its structure.\n",
      "12. Estimate pol: Calculate the polar index of the molecule.\n",
      "13. Estimate BB: Estimate the BB value of the molecule based on its structure.\n",
      "14. Estimate BB: Estimate the BB value of the molecule based on its structure.\n",
      "15. Estimate lipo: Estimate the lipophilic content of the molecule.\n",
      "16. Estimate pol: Calculate the polar index of the molecule.\n",
      "17. Estimate BB: Estimate the BB value of the molecule based on its structure.\n",
      "18. Estimate lipo: Estimate the lipophilic content of the molecule.\n",
      "19. Estimate pol: Calculate the polar index of the molecule.\n",
      "20. Calculate lipo: Estimate the lipophilic content of the molecule.\n",
      "User \n",
      "Synthesize/Time elapsed: 15.044968128204346 seconds\n"
     ]
    }
   ],
   "source": [
    "# **Step 2:** Knowledge synthesis for BBBP dataset\n",
    "print(\"Processing Step 2: LLM for Scientific Synthesis\")\n",
    "%run synthesize_local.py --dataset bbbp --subtask \"\" --model falcon-7b --output_folder \"synthesize_model_response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b719bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Step 3: LLM for Scientific Inference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\m.amintoosi\\.conda\\envs\\pth-gpu\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91755c9380944333b22b0a6e878e74ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# **Step 3:** Knowledge inference for BBBP dataset\n",
    "print(\"Processing Step 3: LLM for Scientific Inference\")\n",
    "%run inference.py --dataset bbbp --subtask \"\" --model falcon-7b --list_num 30 --output_folder \"inference_model_response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6f38f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Step 4:** Summarizing inference rules using GPT-4\n",
    "print(\"Processing Step 4: Summarizing Rules from GPT-4\")\n",
    "API_KEY = \"\"  # Set your OpenAI API Key before running\n",
    "%run summarize_rules.py --input_model_folder falcon-7b --dataset bbbp --subtask \"\" --list_num 30 --api_key $API_KEY --output_folder \"summarized_inference_rules\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9397c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Step 5:** Interpretable model training and evaluation\n",
    "print(\"Processing Step 5: Interpretable Model Training and Evaluation\")\n",
    "%run code_gen_and_eval.py --dataset bbbp --subtask \"\" --model falcon-7b --knowledge_type \"synthesize\" --api_key $API_KEY --output_dir \"llm4sd_results\" --code_gen_folder \"llm4sd_code_generation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fe505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run code_gen_and_eval.py --dataset bbbp --subtask \"\" --model falcon-7b --knowledge_type \"inference\" --list_num 30 --api_key $API_KEY --output_dir \"llm4sd_results\" --code_gen_folder \"llm4sd_code_generation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run code_gen_and_eval.py --dataset bbbp --subtask \"\" --model falcon-7b --knowledge_type \"all\" --list_num 30 --api_key $API_KEY --output_dir \"llm4sd_results\" --code_gen_folder \"llm4sd_code_generation\"\n",
    "\n",
    "print(\"Processing completed for BBBP dataset.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fe9fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df68eb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you're looking to get more specific, here are some tips for optimizing GPU usage on a Jetson Nano:\n",
      "\n",
      "1. **Monitor GPU temps**: High temperatures can limit GPU performance. Use tools like `temp` command or `lspci -vnn` to monitor temperatures.\n",
      "2. **Adjust VRAM allocation**: You may need to adjust the amount of VRAM allocated to each process using the `jetson-config` tool.\n",
      "3. **Compile with Optimize flags**: Compile your applications with optimization flags (-Wl,--gc-sections=-O2) and strip unused symbols.\n",
      "4. **Use less memory-intensive algorithms**: If possible, use algorithms that are more memory-efficient in the first place.\n",
      "5. **Dust-gate removal**: Remove dust from the heatsinks to ensure good airflow inside the device.\n",
      "6. **Keep your system up-to-date**: Regularly update your Jetson OS and drivers to take advantage of performance improvements.\n",
      "\n",
      "Keep in mind, the Jetson Nano is an embedded platform with limited resources, so GPU optimization might not lead to dramatic performance gains compared to more powerful platforms like the Tesla or A100 GPUs found on larger servers.\n",
      "\n",
      "Do you need help with these specifics?\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "class ChatModel:\n",
    "    def __init__(self, base_url, key):\n",
    "        self.client = OpenAI(\n",
    "            base_url=base_url,\n",
    "            api_key=key,\n",
    "        )\n",
    "\n",
    "    def chat_completion(self, model, messages):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages\n",
    "        )\n",
    "        return response\n",
    "\n",
    "BASE_URL = \"http://localhost:11434/v1\"  # Default local URL for Ollama\n",
    "chatModel = ChatModel(base_url=BASE_URL, key=\"fake-key\")  # Key is required but not used by Ollama\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a Jetson-based assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How can I optimize GPU usage on a Jetson Nano?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Use TensorRT for inference and disable services you don't need.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Got it, thanks!\"}\n",
    "]\n",
    "\n",
    "response = chatModel.chat_completion(model=\"llama3.2:latest\", messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be353061",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'choices'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# response = chatModel.chat_completion(model=\"llama3.2:latest\", messages=messages)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# response = chatModel.chat_completion(model=\"gemma3:27b\", messages=messages)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m response \u001b[38;5;241m=\u001b[39m chatModel\u001b[38;5;241m.\u001b[39mchat_completion(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalcon-7b\u001b[39m\u001b[38;5;124m\"\u001b[39m, messages\u001b[38;5;241m=\u001b[39mmessages)\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresponse\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchoices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mKeyError\u001b[0m: 'choices'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "class ChatModel:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "\n",
    "    def chat_completion(self, model, messages):\n",
    "        response = requests.post(\n",
    "            f\"{self.base_url}/chat/completions\",\n",
    "            json={\"model\": model, \"messages\": messages}\n",
    "        )\n",
    "        return response.json()\n",
    "\n",
    "BASE_URL = \"http://localhost:11434/v1\"\n",
    "chatModel = ChatModel(base_url=BASE_URL)\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a Jetson-based assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"How can I optimize GPU usage on a Jetson Nano?\"}\n",
    "# ]\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of Japan?\"}\n",
    "]\n",
    "\n",
    "# response = chatModel.chat_completion(model=\"llama3.2:latest\", messages=messages)\n",
    "# response = chatModel.chat_completion(model=\"gemma3:27b\", messages=messages)\n",
    "response = chatModel.chat_completion(model=\"falcon:7b\", messages=messages)\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f64e939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Blood-brain barrier permeability should be evaluated in animal and preclinical models with in vivo imaging techniques. \n",
      "2. The brain blood-plasma concentration ratios for small molecules should approximate an aqueous solubility value. \n",
      "3. Blood-brain barrier permeability of molecules should correlate with their degree of brain-specific delivery.\n",
      "4. Routes of administration that affect blood-brain barrier delivery should be analyzed.\n",
      "5. Blood-brain barrier permeability of drugs should be evaluated prior to the clinical trial.\n",
      "6. Blood-brain barrier permeability of drugs should be correlated with in vivo data from clinical trials.\n",
      "7. Blood-brain barrier permeability of molecules should be analyzed in human models.\n",
      "8. Blood-brain barrier permeability of small molecules should be evaluated in animal models without brain expression.\n",
      "9. Blood-brain barrier permeability should be predicted using in vitro molecular modeling approaches.\n",
      "10. Blood-brain barrier permeability of small molecules should be evaluated in animal models with brain expression.\n",
      "11. Blood-brain barrier permeability studies should be performed with clinically relevant dosage forms.\n",
      "12. The molecular dimensions of small molecules should be correlated with their permeability properties. \n",
      "13. Blood-brain barrier delivery characteristics of drugs should be analyzed by using blood-brain barrier–permeable carriers.\n",
      "14. Blood-brain barrier properties of drugs should be correlated with their permeability studies in animal models.\n",
      "15. Blood–brain barrier delivery methods should be evaluated with small molecules and drugs.\n",
      "16. Trans-membrane pressure should be used as an indicator to predict blood-brain barrier permeability of small molecules.\n",
      "17. Blood-brain barrier permeability of small molecules that do not have specific therapeutic targets should be analyzed.\n",
      "18. Blood-brain barrier delivery of small therapeutic peptides in vivo studies should be evaluated.\n",
      "19. Brain blood flux should be used to evaluate the blood-brain barrier permeability of drugs. \n",
      "20. Blood-brain barrier permeability should be assessed in animal models with and without a blood-brain barrier.\n",
      "User \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "class ChatModel:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "\n",
    "    def chat_completion(self, model, messages):\n",
    "        response = requests.post(\n",
    "            f\"{self.base_url}/chat/completions\",\n",
    "            json={\"model\": model, \"messages\": messages}\n",
    "        )\n",
    "        return response.json()\n",
    "\n",
    "BASE_URL = \"http://localhost:11434/v1\"\n",
    "chatModel = ChatModel(base_url=BASE_URL)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an experienced chemist specializing in molecular property prediction.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Please come up with 20/30 rules that are important to predict blood–brain barrier permeability.\"}\n",
    "]\n",
    "\n",
    "response = chatModel.chat_completion(model=\"falcon:7b\", messages=messages)\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

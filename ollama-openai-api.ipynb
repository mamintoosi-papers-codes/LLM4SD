{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fe9fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df68eb0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you're looking to get more specific, here are some tips for optimizing GPU usage on a Jetson Nano:\n",
      "\n",
      "1. **Monitor GPU temps**: High temperatures can limit GPU performance. Use tools like `temp` command or `lspci -vnn` to monitor temperatures.\n",
      "2. **Adjust VRAM allocation**: You may need to adjust the amount of VRAM allocated to each process using the `jetson-config` tool.\n",
      "3. **Compile with Optimize flags**: Compile your applications with optimization flags (-Wl,--gc-sections=-O2) and strip unused symbols.\n",
      "4. **Use less memory-intensive algorithms**: If possible, use algorithms that are more memory-efficient in the first place.\n",
      "5. **Dust-gate removal**: Remove dust from the heatsinks to ensure good airflow inside the device.\n",
      "6. **Keep your system up-to-date**: Regularly update your Jetson OS and drivers to take advantage of performance improvements.\n",
      "\n",
      "Keep in mind, the Jetson Nano is an embedded platform with limited resources, so GPU optimization might not lead to dramatic performance gains compared to more powerful platforms like the Tesla or A100 GPUs found on larger servers.\n",
      "\n",
      "Do you need help with these specifics?\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "class ChatModel:\n",
    "    def __init__(self, base_url, key):\n",
    "        self.client = OpenAI(\n",
    "            base_url=base_url,\n",
    "            api_key=key,\n",
    "        )\n",
    "\n",
    "    def chat_completion(self, model, messages):\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages\n",
    "        )\n",
    "        return response\n",
    "\n",
    "BASE_URL = \"http://localhost:11434/v1\"  # Default local URL for Ollama\n",
    "chatModel = ChatModel(base_url=BASE_URL, key=\"fake-key\")  # Key is required but not used by Ollama\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a Jetson-based assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How can I optimize GPU usage on a Jetson Nano?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Use TensorRT for inference and disable services you don't need.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Got it, thanks!\"}\n",
    "]\n",
    "\n",
    "response = chatModel.chat_completion(model=\"llama3.2:latest\", messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be353061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Japan is **Tokyo**. \n",
      "\n",
      "It's not only the capital, but also the most populous metropolis in the world! ðŸ˜Š \n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "class ChatModel:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "\n",
    "    def chat_completion(self, model, messages):\n",
    "        response = requests.post(\n",
    "            f\"{self.base_url}/chat/completions\",\n",
    "            json={\"model\": model, \"messages\": messages}\n",
    "        )\n",
    "        return response.json()\n",
    "\n",
    "BASE_URL = \"http://localhost:11434/v1\"\n",
    "chatModel = ChatModel(base_url=BASE_URL)\n",
    "\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": \"You are a Jetson-based assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"How can I optimize GPU usage on a Jetson Nano?\"}\n",
    "# ]\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of Japan?\"}\n",
    "]\n",
    "\n",
    "# response = chatModel.chat_completion(model=\"llama3.2:latest\", messages=messages)\n",
    "response = chatModel.chat_completion(model=\"gemma3:27b\", messages=messages)\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
